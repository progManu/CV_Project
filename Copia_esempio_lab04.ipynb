{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Copia esempio lab04 per verificare se ci fosse un'errore nel nostro training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2k45FnyiHU_b",
        "outputId": "5a89c8c3-0ece-45b5-a5e5-90533f3ffbc9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\goate\\miniconda3\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] Impossibile trovare la procedura specificata'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
            "  warn(\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import cv2\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.utils.data import random_split\n",
        "from PIL import Image\n",
        "import copy\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "pYshmxDoSZ69"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    # you can add other transformations in this list\n",
        "    transforms.ConvertImageDtype(torch.float32),\n",
        "    transforms.Resize((64, 64))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdbPeGzQHvDE",
        "outputId": "5e255877-8b9c-4ba1-ec01-2470c8b250e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "entire train folder: 1500, entire test folder: 2985, splitted trainset: 1275,  splitted validset: 225\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 32\n",
        "\n",
        "def open_image(img):\n",
        "    np_array = cv2.imread(filename=img, flags=cv2.IMREAD_GRAYSCALE)\n",
        "    width, height = np_array.shape\n",
        "    t = torch.from_numpy(np_array)\n",
        "    t = torch.reshape(t, (1, width, height))\n",
        "    return t\n",
        "\n",
        "trainset = torchvision.datasets.ImageFolder(root='CVPR2023_project_2_and_3_data/train/', transform=transform, loader=open_image)\n",
        "\n",
        "true_train_set, validation_train_set = random_split(trainset, (0.85, 0.15))\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(true_train_set, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True) # batch size of 1 because we have to crop in order to get all images to same size (64x64), also see pin_memory optin\n",
        "validloader = torch.utils.data.DataLoader(validation_train_set, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)\n",
        "\n",
        "testset = torchvision.datasets.ImageFolder(root='CVPR2023_project_2_and_3_data/test/', transform=transform, loader=open_image)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True) # batch size of 1 because we have to crop in order to get all images to same size (64x64), also see pin_memory optin\n",
        "\n",
        "print(f'entire train folder: {len(trainset)}, entire test folder: {len(testset)}, splitted trainset: {len(true_train_set)},  splitted validset: {len(validation_train_set)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Hf34ACebUfXB"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTOg9GzyKpmu",
        "outputId": "abb5788f-01b5-4e55-af66-7adca264240f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CNN(\n",
              "  (conv): Sequential(\n",
              "    (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (4): ReLU()\n",
              "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (6): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (7): ReLU()\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Flatten(start_dim=1, end_dim=-1)\n",
              "    (1): Linear(in_features=4608, out_features=15, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class CNN(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv = torch.nn.Sequential(\n",
        "                torch.nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3, stride=1),\n",
        "                torch.nn.ReLU(),\n",
        "                torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "                torch.nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=1),\n",
        "                torch.nn.ReLU(),\n",
        "                torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "                torch.nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1),\n",
        "                torch.nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.classifier = torch.nn.Sequential(\n",
        "            torch.nn.Flatten(),\n",
        "            torch.nn.Linear(32*12*12, 15)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.classifier(self.conv(x))\n",
        "\n",
        "\n",
        "model = CNN().to(device)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CNN(\n",
              "  (conv): Sequential(\n",
              "    (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (4): ReLU()\n",
              "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (6): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (7): ReLU()\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Flatten(start_dim=1, end_dim=-1)\n",
              "    (1): Linear(in_features=4608, out_features=15, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def initialize_weigths_and_bias(m):\n",
        "  if (type(m) == torch.nn.Linear) or (type(m) == torch.nn.Conv2d):\n",
        "      torch.nn.init.constant_(m.bias.data, 0) # initialize weights\n",
        "      torch.nn.init.normal_(m.weight.data, mean=0.0, std=0.01)\n",
        "  \n",
        "\n",
        "model.apply(initialize_weigths_and_bias)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZxrRJsi2Ouu",
        "outputId": "0e3799f5-2a65-4226-8d7a-66cbd5b857d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loop...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\goate\\miniconda3\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/30], Loss: 2.709\n",
            "Epoch [2/30], Loss: 2.709\n",
            "Epoch [3/30], Loss: 2.708\n",
            "Epoch [4/30], Loss: 2.708\n",
            "Epoch [5/30], Loss: 2.708\n",
            "Epoch [6/30], Loss: 2.708\n",
            "Epoch [7/30], Loss: 2.708\n",
            "Epoch [8/30], Loss: 2.709\n",
            "Epoch [9/30], Loss: 2.708\n",
            "Epoch [10/30], Loss: 2.709\n",
            "Epoch [11/30], Loss: 2.708\n",
            "Epoch [12/30], Loss: 2.708\n",
            "Epoch [13/30], Loss: 2.708\n",
            "Epoch [14/30], Loss: 2.708\n",
            "Epoch [15/30], Loss: 2.708\n",
            "Epoch [16/30], Loss: 2.708\n",
            "Epoch [17/30], Loss: 2.709\n",
            "Epoch [18/30], Loss: 2.709\n",
            "Epoch [19/30], Loss: 2.708\n",
            "Epoch [20/30], Loss: 2.708\n",
            "Epoch [21/30], Loss: 2.708\n",
            "Epoch [22/30], Loss: 2.708\n",
            "Epoch [23/30], Loss: 2.708\n",
            "Epoch [24/30], Loss: 2.708\n",
            "Epoch [25/30], Loss: 2.708\n",
            "Epoch [26/30], Loss: 2.708\n",
            "Epoch [27/30], Loss: 2.708\n",
            "Epoch [28/30], Loss: 2.708\n",
            "Epoch [29/30], Loss: 2.708\n",
            "Epoch [30/30], Loss: 2.708\n"
          ]
        }
      ],
      "source": [
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "def train_one_epoch(epoch_index, loader):\n",
        "  running_loss = 0\n",
        "\n",
        "  for i,data in enumerate(loader):\n",
        "\n",
        "    input, labels = data\n",
        "    input = input.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    outputs = model(input)\n",
        "\n",
        "    loss = loss_function(outputs, labels)\n",
        "    running_loss += loss.item()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  return running_loss/(i+1)\n",
        "\n",
        "EPOCHS = 30\n",
        "\n",
        "print('Training loop...')\n",
        "for epoch in range(EPOCHS):\n",
        "  train_loss = train_one_epoch(epoch, trainloader)\n",
        "  print(f'Epoch [{epoch + 1}/{EPOCHS}], Loss: {train_loss:.3f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "9AkCaAq6Lbi8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the test images: 4 %\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the trained network on the test set\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in validloader:\n",
        "        images, labels = data\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the test images: %d %%' % (\n",
        "    100 * correct / total))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
